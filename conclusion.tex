\chapter{SUMMARY AND FUTURE WORK}
\label{chap:conclusion}
In this chapter, we summarize some experimental results of \Cref{chap:experiments}, as well as provide an outline for further improvements, both with respect to optimizations and new features.

\section{Summary of Results}
\label{sec:summary}
In this work, we attempt to accelerate genetic programming based algorithms, for symbolic regression and transformation on the GPU. We do this by parallelizing the selection and evaluation step of the generational GP algorithm. We introduce a prefix-list representation for expression trees, which are then evaluated using an optimized stack on the GPU. Fully vectorized routines for standard loss functions are also provided, which can be used as fitness functions during the training of a genetic population of programs. At the end of a run, our algorithm returns the entire set of evolved programs for all generations. One can filter the most optimal program from the last generation of programs, or the programs with maximum variation (based on fitness values) to perform symbolic regression or transformation respectively. 

On synthetic datasets for the Pagie Polynomial ranging in size from $4096$ to $4$ million points, we were able to achieve an average speedup of $27\times$ with respect to gplearn\citep{gplearn}, a library our implementation takes inspiration from. Our algorithm evolves a set of $50$ programs for $50$ generations on a $4$ million rows in less than $10$ seconds. 
% Even the secret Joestar technique can't compare to this lol

\section{Further Improvements}
\label{sec:improvements}
In this section, we list out a few possible feature improvements and optimizations to our algorithm's implementation.

\subsection{Possible Optimizations}
\label{subsec:optimizations}
In our experiments, we found that for large datasets, maximum amount of GPU time (around $71.5\%$) is spent in the evaluation kernel. However, this was an expected figure, as evaluation is the main bottleneck for many GP programs. 

We note that as long as an exact copy of the population program exists, it doesn't matter if the representation of the program slightly changes on the device. Keeping this in mind, one can attempt to reduce the average depth of the trees being evaluated by performing some balancing on the device trees(while still maintaining the prefix order for all expressions). This will help in the evaluation step, as lesser stack slots will be used for a small-depth balanced tree compared to a larger depth unbalanced tree, leading to lower kernel stack memory usage per thread. However, we note that implementing tree balancing in the GPU is a non-trivial task.

Note that another optimization approach is to convert the current prefix-list into a DAG to cache values of common sub-expressions. However, in this case, the evaluation method of the underlying tree itself changes. 

\subsection{New Features}
\label{subsec:newfeatures}

Our implementation of genetic programming in cuML lacks a few features compared to other standard GP libraries. We discuss a few of these missing features, along with possible implementation strategies in the current scheme. 

\subsubsection{Supporting custom function sets}
Our current implementation of genetic programming in cuML provides a constant list of $33$ functions. We don't expose any way for users to define their own custom functions for use during evolution. 

Support for this can be added through the use of \lstinline!__host__ __device__! function pointers. The user can specify a list of function pointers as training hyper parameters. During the stack execution phase, the specified function pointer is called at runtime for evaluation.

\subsubsection{Custom Loss functions}

Since CUDA supports \lstinline!__host__ __device__! Lambda functions, custom loss functions are easier to support compared to custom function sets. The user can specify the loss function for a single predicted label in a lambda function. Vectorization of this loss function over the program population and input dataset can be done in a CUDA kernel to which the lambda function is passed as input. 
