\chapter{OUR WORK}
\label{chap:ourwork}
This chapter contains the implementation details of our parallel algorithm to perform genetic programming using CUDA. We first talk about a way to represent programs on the GPU. This is then followed by a description of the device side data structures used. We then give an overview of our modified GP algorithm, describing GPU-side optimizations for the selection, and evaluation step in detail. We also include details about the fitness computation step which comes after the evaluation step. Finally, we talk about the various challenges faced during the implementation of the modified algorithm, and the workarounds to avoid these problems.

In this implementation, we use a fixed list of functions with a maximum arity of $2$. We also assume that the maximum depth of all expression trees is a constant. This assumption is needed in order to evaluate the trees in the GPU using a fixed size stack.

\section{Program Representation}
\label{ow:input}
We define a struct for a program containing the following components. 
\begin{itemize}
    \item A array of operators and operands of the underlying expression tree stored in Polish(prefix) notation.
    \item A length parameter corresponding to the total number of nodes in the expression tree. 
    \item A raw fitness parameter containing the score of the current tree on the input data-set. 
    \item The depth of the current expression tree. 
\end{itemize}

The entire population for a given generation is thus stored in the Array of Structures(AoS) format. The evaluation using a stack is almost similar to the way the Push3 system \citep{push3Stack} evaluates GP programs, with the sole exception being the reverse iteration due to the prefix notation chosen for the trees. 

Prefix notation was used for the representation of nodes to aid with the process of generating random programs, where we directly generate a valid prefix tree on the CPU.

\subsection{Device Side Data Structures}
\label{ow:deviceds}
In order to perform tournament selection and evaluation on the GPU, we use the following device side data-structures. 
\begin{itemize}
    \item \textbf{Philox Random Number Generator(RNG)} - We use a Philox counter-based RNG\citep{Philox2011} implemented in the {\bf raft} library \citep{raschka2020machine} to generate random global indices for tournament selection inside the selection kernel. 
    \item \textbf{Fixed size device stack} - We define a fixed size stack using a custom class, implementing the \textit{push}, \textit{pop} methods as inline \lstinline!__host__ __device__! functions. To avoid global memory accesses and encourage register look-ups for internal stack slots, the push and pop operation is implemented using an unrolled loop over all the available slots. This action is possible because the maximum size of the stack is fixed at $20$ for now. We shall discuss more about this in \cref{sec:challenges}, where we examine how to avoid thread divergence in this setup.
\end{itemize}

Our kernels for both selection and program execution have been written in a way so as to eliminate any need for thread or memory synchronization. 

\subsection{Memory Footprint}
\label{ow:memory}
We examine the memory footprint of our implementation with respect to the number of programs stored in both CPU and GPU memory below. 
\begin{itemize}
    \item \textbf{CPU} - Depending on a user-specified flag, we maintain a history of all the programs for all generations. If this is not desired, then we only store information about $2$ generations, the current and the next generation until the end.
    \item \textbf{GPU} - Only the device memory corresponding to the current and the next generation of programs is stored through the run of the algorithm. During the course of the algorithm, the $2$ memory locations are updated with new programs in a ping pong fashion.
\end{itemize}

We will now explain our code for implementing parallel GP in \cref*{ow:paralgo}.
\section{Our Parallel GP Algorithm}
\label{ow:paralgo}
In this section, we again outline the individual steps of \Cref{gpalgo}, defined in \Cref{bgrw:algo}. However, each step contains details specific to our implementation. In this implementation, the selection and evaluation step is performed on the GPU, whereas mutations are carried out on the CPU.

Before performing any of the standard steps, we decide on the type of mutation through which the next generation program is produced. This mutation type selection is governed by user defined probabilities for the various types of mutations. This step is important, as we need to determine the exact  number of selection tournaments to be run(as crossovers require $2$ tournaments to decide the parent and donor trees). 

Once exact number of tournaments has been decided, we move on to the selection step.

\subsection{Selection}
\label{ow:selection}
We perform selection by running parallel tournaments in a CUDA kernel. For a population of size $n$ and tournament subset size $k$, we launch a CUDA kernel with $\left\lfloor n/256 \right\rfloor $ blocks and $256$ threads.In each thread, we generate $k$ random program indices using a Philox RNG(see \cref{ow:deviceds}). 

The index of the best program among the $k$ indices (based on the raw fitness values of the device programs) is then chosen as the winning index for the given thread, and is recorded in a \lstinline!win_indices! array.

\subsection{Mutation}
\label{ow:mutation}
The mutation of programs takes place on the CPU itself. We implement all the mutations mentioned in \cref{subsec:mutation} with a slight modification to the crossover operation in order to constrain the depth of the output tree. We call this modification a hoisted crossover.  

\subsubsection{Hoisted Crossover}

In this mutation, we initially perform a crossover between the parent tree and the donor tree. The selected stubtree of the donor is then repeatedly hoisted onto the parent tree until the depth of the resultant tree is less than the maximum evaluation stack size. \\
Note that the hoist mutation occurs only on the donor subtree, since that is the part of the tree which contributes to the depth violation. If this was not the case, then another subtree of the parent subtree would have contributed towards the max depth violation, making the parent program itself an invalid one. \\
This modification is necessary for our code since a stack of fixed size $m$ can only evaluate a tree of depth $m-1$(assuming the maximum function arity is $2$)

At the end of mutations, we allocate and transfer the newly created programs onto GPU memory, in order to evaluate them on the input dataset. In order to save on device memory, we also deallocate the GPU memory of the previous population trees. Some of the challenges we faced due to the nested nature of programs during the \lstinline!cudaMemcpy! operations between host and device memory are listed in \cref{sec:challenges}. 

\subsection{Evaluation}
\label{ow:evaluation}
We divide the evaluation portion into $2$ steps, namely execution and metric computation. In the execution step, all programs in the new population are evaluated on the given dataset, to produce a set of predictions. Once we have the predicted values for all programs, we compute the raw fitness value of the every program with respect to the expected output using the user-defined metric. These steps are explained in detail below.  

\subsubsection{Execution}
\label{subsec:execute}
For every program in the population and every sample in the input dataset, we produce predicted output values in this step. If $n$ is the population size, and $m$ is the number of samples in the input dataset, then we launch an execution kernel with a 2D grid of dimension $(\left\lceil m/256\right\rceil ,n)$ with $256$ threads per block. Each thread has it's own device side stack which evaluates a program on a specific row.

We note here that to avoid thread divergence in the execution kernel, it is important to ensure that each block of threads execute on different rows of the same program. We provide more details about this in \cref*{sec:challenges}.

\subsubsection{Fitness Metric Computation}
\label{subsec:fitness}
In the previous step, after running the execution kernel, for all programs in the population, we have a list of predicted values. We also have access to the list of actual outputs. 

For every program, we compute fitness using a user-selected loss function. The inputs to the loss function are the program's output values(from the execution step) and the actual outputs.

Since the fitness computation is the same for all programs, we computed loss for all programs in parallel on the GPU. This was implemented using the matrix-vector linear algebra primitives present in the \textbf{raft} library\citep{raschka2020machine}, where the vector of actual output values is reused for all predicted values of different programs. We implement a weighted version of the following $6$ standard loss functions:
\begin{itemize}
    \item Mean Absolute Error (MAE)
    \item Mean Square Error (MSE)
    \item Root Mean Square Error (RMSE)
    \item Logistic Loss(binary loss only)
    \item Karl Pearson's Correlation Coefficient
    \item Spearman's Rank Correlation Coefficient
\end{itemize}
During the implementation of Spearman Rank Correlation, we used the \textbf{thrust} library from Nvidia to generate ranks for the given values. The default fitness function is set as MAE, in an effort to be consistent with \textbf{gplearn} \citep{gplearn}

\section{Challenges Faced}
\label{sec:challenges}
In this section, we detail in brief some of the challenges faced during the implementation of our parallel GP algorithm.


\subsection{Thread divergence and Global Memory Access}
\label{prob:divergence}
In the execution kernel, during the evaluation phase, when evaluating a stack node, we check for equality of the current function with $33$ pre-defined functions, using \lstinline!if-else! conditions on the node type. 

Since CUDA executes statements using warps of $32$ threads in parallel, when it encounters an \lstinline!if-else! block inside a kernel which is triggered only for a subset of the warp, both the \lstinline!if! and \lstinline!else! block are executed by all threads. During the execution of the \lstinline!if! block, the threads which don't trigger the \lstinline!if! condition are masked, but still consume resources, with the same behaviour exhibited for the \lstinline!else! block. This increases the total execution time as both blocks are effectively processed by all warp threads. 

To avoid this behaviour in our code, we ensure that within every thread-block of the execution kernel, all threads execute the same program. This ensures that all threads in a warp will always parallely execute the same nodes, and thus avoid divergence during a single stack evaluation.

In the implementation of \textbf{push} and \textbf{pop} operations for the device side stack, we avoid trying a possible dereference using global memory index(the current number of elements in the stack) by using an unrolled loop for stack memory access. This is again safe from thread divergence because we ensure that within a thread-block, all threads evaluate the same program. 

\subsection{Data Transfers and Memory Allocation}
\label{prob:memcpy}
Since we went with a AoS representation for the list of programs and each program has a nested pointer for the list of nodes in it, we are forced to perform at least $2$ \lstinline!cudaMemcpy! operations per program in a loop spanning the population size. One of the copy operations is for the list of program nodes, and the other copy is to capture the metadata about the nodes, and the other copy is for the program struct itself. 

However, during our experiments, we achieved a considerable speedup despite this step. Thus, the implementation was not altered.

In the next chapter, we will describe some benchmark experiments and present our results. 





